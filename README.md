# Pyspark Project

Welcome to the Pyspark tutorial section.

The courses comprises of 4 folders containing notebooks. Folders and notebooks are sorted in order of difficulty given their name, so you should follow the numerotation. For example, you should finish all notebooks in `1-beginner` before starting `2-novice`. Likewise, when doing `2-novice` finish the `1-...` notebook before doing `2-...`.

Inside each notebook, we have documented a number of questions and unimplemented code cells answering the question, followed by a code cell which acts as test cases for the function called `Graded cell`. Your submission will be graded on how many test cases you pass given your implementation of the previous function. 

**PS**: The instructor has hidden test cases on his side so don't try to circumvent the system by just returning the expected value in your method.

<h2 id="Prerequisites">ğŸ¤– Prerequisites</h2>

- [Anaconda 2019+](https://www.anaconda.com/download/)
- Required Java 8/11. 
    - You can set the `JAVA_HOME` environment variable to point to the Java 8/11 folder you want to use for the project, from `Edit the system environment variables` window or `set JAVA_HOME=<path_to_java>` in command-line before running `jupyter notebook`. 
    - You may also install Java OpenJDK **inside** your Anaconda environment with `conda install openjdk`. The `JAVA_HOME` variable should be automatically updated for this environment only.


<h2 id="Project-structure">ğŸ—ï¸ Project structure</h2>

```
â”œâ”€â”€ 1-beginner/
â”‚   â”œâ”€â”€ 1-Initiation.ipynb
â”‚   â””â”€â”€ jupyter.png
â”œâ”€â”€ 2-novice/
â”‚   â”œâ”€â”€ 1-Initiation-RDD.ipynb
â”‚   â”œâ”€â”€ 2-Pagerank-RDD.ipynb
â”‚   â”œâ”€â”€ FL_insurance_sample.csv
â”‚   â””â”€â”€ pagerank.png
â”œâ”€â”€ 3-advanced/
â”‚   â”œâ”€â”€ 1-Initiation-SparkSQL.ipynb
â”‚   â”œâ”€â”€ 2-Advanced-SQL-and-ML.ipynb
â”‚   â”œâ”€â”€ FL_insurance_sample.csv
â”‚   â””â”€â”€ titanic.csv
â”œâ”€â”€ cheatsheets/
â”‚   â”œâ”€â”€ conda-cheatsheet.pdf
â”‚   â”œâ”€â”€ Jupyter-notebook.pdf
â”‚   â”œâ”€â”€ Pyspark-RDD.pdf
â”‚   â”œâ”€â”€ Pyspark-SQL.pdf
â”‚   â””â”€â”€ Python.pdf
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ custom.css
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ intro.pdf
â”‚   â””â”€â”€ pyspark.pdf  
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

<h2 id="Installation">ğŸ› ï¸ Installation</h2>

To install the project on your local machine, you can use the following command :
```
https://github.com/karamoko17/Projet_pyspark.git
```
Then move to the project directory and create a new conda environment with the following command :
```sh
conda create -n pyspark-tutorial python=3.11
conda activate pyspark-tutorial
pip install -r requirements.txt
# pip install bokeh jupyter numpy pandas psutil pyspark seaborn
```
We provide you with a requirements.txt which is used to download dependencies in a conda environment we will name pyspark-tutorial.

You can run the Jupyter notebook now.

<h2 id="Contribution">ğŸ¤ Contribution</h2>

Contributions are welcome! Feel free to open a ticket or submit a pull request to suggest improvements. Here's how you can get involved:

1. Clone the project.
2. Create a branch for your feature.
3. Make your changes and validate them with a clear message.
4. Push your changes to your branch on the remote repository.  
5. Submit a pull request to have your contribution reviewed.

<h2 id="Author">ğŸ¯ Author</h2> 
This project was designed and developed by KARAMOKO Awa, a student in Master 2 SISE (Statistics and Computer Science for Data Science) at UniversitÃ© LumiÃ¨re Lyon 2.
